{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24cebd371702429d91b30769b2ef5a22",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.svâ€¦"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (4.39.2)\n",
      "Collecting transformers\n",
      "  Downloading transformers-4.40.1-py3-none-any.whl.metadata (137 kB)\n",
      "     ---------------------------------------- 0.0/138.0 kB ? eta -:--:--\n",
      "     ----- ------------------------------- 20.5/138.0 kB 330.3 kB/s eta 0:00:01\n",
      "     ---------------- -------------------- 61.4/138.0 kB 544.7 kB/s eta 0:00:01\n",
      "     -------------------------------------- 138.0/138.0 kB 1.0 MB/s eta 0:00:00\n",
      "Requirement already satisfied: accelerate in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (0.28.0)\n",
      "Collecting accelerate\n",
      "  Downloading accelerate-0.29.3-py3-none-any.whl.metadata (18 kB)\n",
      "Requirement already satisfied: filelock in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (3.13.3)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2023.12.25)\n",
      "Requirement already satisfied: requests in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (2.31.0)\n",
      "Collecting tokenizers<0.20,>=0.19 (from transformers)\n",
      "  Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (0.4.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from transformers) (4.66.2)\n",
      "Requirement already satisfied: psutil in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (5.9.8)\n",
      "Requirement already satisfied: torch>=1.10.0 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from accelerate) (2.2.2+cu118)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (2024.2.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from huggingface-hub<1.0,>=0.19.3->transformers) (4.10.0)\n",
      "Requirement already satisfied: sympy in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (1.12)\n",
      "Requirement already satisfied: networkx in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from torch>=1.10.0->accelerate) (3.1.3)\n",
      "Requirement already satisfied: colorama in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm>=4.27->transformers) (0.4.6)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (3.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->transformers) (2024.2.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.5)\n",
      "Requirement already satisfied: mpmath>=0.19 in c:\\users\\wen-liang\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\n",
      "Downloading transformers-4.40.1-py3-none-any.whl (9.0 MB)\n",
      "   ---------------------------------------- 0.0/9.0 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.2/9.0 MB 15.7 MB/s eta 0:00:01\n",
      "   - -------------------------------------- 0.4/9.0 MB 3.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.5/9.0 MB 3.9 MB/s eta 0:00:03\n",
      "   -- ------------------------------------- 0.6/9.0 MB 3.5 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.7/9.0 MB 3.3 MB/s eta 0:00:03\n",
      "   --- ------------------------------------ 0.8/9.0 MB 3.2 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.0/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ---- ----------------------------------- 1.1/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ----- ---------------------------------- 1.3/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.4/9.0 MB 3.1 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.4/9.0 MB 3.0 MB/s eta 0:00:03\n",
      "   ------ --------------------------------- 1.6/9.0 MB 2.8 MB/s eta 0:00:03\n",
      "   ------- -------------------------------- 1.7/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 1.8/9.0 MB 2.8 MB/s eta 0:00:03\n",
      "   -------- ------------------------------- 2.0/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   --------- ------------------------------ 2.1/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.3/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ---------- ----------------------------- 2.4/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.5/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ----------- ---------------------------- 2.7/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------ --------------------------- 2.8/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 2.9/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   ------------- -------------------------- 3.1/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.2/9.0 MB 2.9 MB/s eta 0:00:03\n",
      "   -------------- ------------------------- 3.4/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   --------------- ------------------------ 3.5/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.7/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 3.8/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ----------------- ---------------------- 3.9/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.1/9.0 MB 2.9 MB/s eta 0:00:02\n",
      "   ------------------ --------------------- 4.2/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------- -------------------- 4.4/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.5/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------- ------------------- 4.7/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   --------------------- ------------------ 4.8/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.0/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ---------------------- ----------------- 5.2/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ----------------------- ---------------- 5.3/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.5/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------ --------------- 5.6/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   ------------------------- -------------- 5.8/9.0 MB 3.0 MB/s eta 0:00:02\n",
      "   -------------------------- ------------- 5.9/9.0 MB 3.1 MB/s eta 0:00:02\n",
      "   --------------------------- ------------ 6.1/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------- ------------ 6.2/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 6.4/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.6/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 6.7/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------ --------- 6.9/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.1/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 7.2/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 7.4/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 7.5/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.7/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ---------------------------------- ----- 7.8/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 8.0/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.2/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 8.3/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 8.5/9.0 MB 3.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 8.6/9.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.8/9.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------  8.9/9.0 MB 3.2 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 9.0/9.0 MB 3.2 MB/s eta 0:00:00\n",
      "Downloading accelerate-0.29.3-py3-none-any.whl (297 kB)\n",
      "   ---------------------------------------- 0.0/297.6 kB ? eta -:--:--\n",
      "   ----------------------- ---------------- 174.1/297.6 kB 5.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 297.6/297.6 kB 3.6 MB/s eta 0:00:00\n",
      "Downloading tokenizers-0.19.1-cp312-none-win_amd64.whl (2.2 MB)\n",
      "   ---------------------------------------- 0.0/2.2 MB ? eta -:--:--\n",
      "   -- ------------------------------------- 0.1/2.2 MB 8.3 MB/s eta 0:00:01\n",
      "   ----- ---------------------------------- 0.3/2.2 MB 4.4 MB/s eta 0:00:01\n",
      "   ------- -------------------------------- 0.4/2.2 MB 3.9 MB/s eta 0:00:01\n",
      "   ---------- ----------------------------- 0.6/2.2 MB 3.7 MB/s eta 0:00:01\n",
      "   ------------- -------------------------- 0.7/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 0.9/2.2 MB 3.5 MB/s eta 0:00:01\n",
      "   ------------------- -------------------- 1.1/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.2/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.4/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------- ----------- 1.6/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ------------------------------- -------- 1.8/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ----------------------------------- ---- 1.9/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 2.1/2.2 MB 3.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.2/2.2 MB 3.5 MB/s eta 0:00:00\n",
      "Installing collected packages: tokenizers, accelerate, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.15.2\n",
      "    Uninstalling tokenizers-0.15.2:\n",
      "      Successfully uninstalled tokenizers-0.15.2\n",
      "  Attempting uninstall: accelerate\n",
      "    Found existing installation: accelerate 0.28.0\n",
      "    Uninstalling accelerate-0.28.0:\n",
      "      Successfully uninstalled accelerate-0.28.0\n",
      "  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.39.2\n",
      "    Uninstalling transformers-4.39.2:\n",
      "      Successfully uninstalled transformers-4.39.2\n",
      "Successfully installed accelerate-0.29.3 tokenizers-0.19.1 transformers-4.40.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\Wen-Liang\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\~okenizers'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "!pip install -U transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n",
      "`flash-attention` package not found, consider installing for better performance: No module named 'flash_attn'.\n",
      "Current `flash-attenton` does not support `window_size`. Either upgrade or use `attn_implementation='eager'`.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a3609909ce3d41e7b4035f98608792a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    \"microsoft/Phi-3-mini-128k-instruct\",\n",
    "    torch_dtype=\"auto\",\n",
    "    trust_remote_code=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "chat = [\n",
    "    {\n",
    "        \"role\":\"user\",\n",
    "        \"content\":\"How ai shake hands with quantum computing\",\n",
    "    }\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[    1, 32010, 29871,    13,  5328,  7468,   528,  1296,  6567,   411,\n",
       "         12101, 20602, 32007, 29871,    13, 32001, 29871,    13]])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt = tokenizer.apply_chat_template(chat, tokenize = False, add_generation_prompt = True)\n",
    "\n",
    "token_ids = tokenizer.encode(prompt, add_special_tokens=False, return_tensors=\"pt\")\n",
    "\n",
    "token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are not running the flash-attention implementation, expect numerical differences.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "\"In the realm of quantum computing, AI doesn't literally 'shake hands.' However, they work symbiotically. AI can be used to interpret and analyze data from quantum computers, while quantum computing can accelerate AI processes, especially in areas such as machine learning and data analysis. This association can be metaphorically seen as different entities 'shaking hands' or working together for mutual benefit.\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "  output_ids = model.generate(\n",
    "      token_ids.to(model.device),\n",
    "      do_sample = True,\n",
    "      temperature = 0.7,\n",
    "      max_new_tokens = 512\n",
    "  )\n",
    "\n",
    "output = tokenizer.decode(output_ids.tolist()[0][token_ids.size(1):], skip_special_tokens=True)\n",
    "\n",
    "output"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://huggingface.co/microsoft/Phi-3-mini-128k-instruct/blob/main/sample_finetune.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import logging\n",
    "\n",
    "import datasets\n",
    "from datasets import load_dataset\n",
    "from peft import LoraConfig\n",
    "import torch\n",
    "import transformers\n",
    "from trl import SFTTrainer\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, TrainingArguments, BitsAndBytesConfig\n",
    "from huggingface_hub import HfApi, HfFolder, Repository, create_repo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "logger = logging.getLogger(__name__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_config = {\n",
    "    \"bf16\": True,\n",
    "    \"do_eval\": False,\n",
    "    \"learning_rate\": 5.0e-06,\n",
    "    \"log_level\": \"info\",\n",
    "    \"logging_steps\": 20,\n",
    "    \"logging_strategy\": \"steps\",\n",
    "    \"lr_scheduler_type\": \"cosine\",\n",
    "    \"num_train_epochs\": 1,\n",
    "    \"max_steps\": -1,\n",
    "    \"output_dir\": \"./sample_phi3_finetune_example\",\n",
    "    \"overwrite_output_dir\": True,\n",
    "    \"per_device_eval_batch_size\": 4,\n",
    "    \"per_device_train_batch_size\": 4,\n",
    "    \"remove_unused_columns\": True,\n",
    "    \"save_steps\": 100,\n",
    "    \"save_total_limit\": 1,\n",
    "    \"seed\": 0,\n",
    "    \"gradient_checkpointing\": True,\n",
    "    \"gradient_checkpointing_kwargs\":{\"use_reentrant\": False},\n",
    "    \"gradient_accumulation_steps\": 1,\n",
    "    \"warmup_ratio\": 0.2,\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "peft_config = {\n",
    "    \"r\": 16,\n",
    "    \"lora_alpha\": 32,\n",
    "    \"lora_dropout\": 0.05,\n",
    "    \"bias\": \"none\",\n",
    "    \"task_type\": \"CAUSAL_LM\",\n",
    "    \"target_modules\": \"all-linear\",\n",
    "    \"modules_to_save\": None,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_conf = TrainingArguments(**training_config)\n",
    "peft_conf = LoraConfig(**peft_config)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:31 - WARNING - __main__ - Process rank: 0, device: cuda:0, n_gpu: 1 distributed training: True, 16-bits training: False\n",
      "2024-06-03 13:27:31 - INFO - __main__ - Training/evaluation parameters TrainingArguments(\n",
      "_n_gpu=1,\n",
      "accelerator_config={'split_batches': False, 'dispatch_batches': None, 'even_batches': True, 'use_seedable_sampler': True, 'non_blocking': False, 'gradient_accumulation_kwargs': None},\n",
      "adafactor=False,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.999,\n",
      "adam_epsilon=1e-08,\n",
      "auto_find_batch_size=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "data_seed=None,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=0,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=[],\n",
      "deepspeed=None,\n",
      "disable_tqdm=False,\n",
      "dispatch_batches=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_steps=None,\n",
      "eval_strategy=IntervalStrategy.NO,\n",
      "evaluation_strategy=None,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "fsdp=[],\n",
      "fsdp_config={'min_num_params': 0, 'xla': False, 'xla_fsdp_v2': False, 'xla_fsdp_grad_ckpt': False},\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "gradient_accumulation_steps=1,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs={'use_reentrant': False},\n",
      "greater_is_better=None,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=False,\n",
      "hub_strategy=HubStrategy.EVERY_SAVE,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_data_skip=False,\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "learning_rate=5e-06,\n",
      "length_column_name=length,\n",
      "load_best_model_at_end=False,\n",
      "local_rank=0,\n",
      "log_level=info,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=./sample_phi3_finetune_example\\runs\\Jun03_13-27-31_LAPTOP-6VMJLCKR,\n",
      "logging_first_step=False,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=20,\n",
      "logging_strategy=IntervalStrategy.STEPS,\n",
      "lr_scheduler_kwargs={},\n",
      "lr_scheduler_type=SchedulerType.COSINE,\n",
      "max_grad_norm=1.0,\n",
      "max_steps=-1,\n",
      "metric_for_best_model=None,\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "num_train_epochs=1,\n",
      "optim=OptimizerNames.ADAMW_TORCH,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "output_dir=./sample_phi3_finetune_example,\n",
      "overwrite_output_dir=True,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=4,\n",
      "per_device_train_batch_size=4,\n",
      "prediction_loss_only=False,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "ray_scope=last,\n",
      "remove_unused_columns=True,\n",
      "report_to=[],\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "run_name=./sample_phi3_finetune_example,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=100,\n",
      "save_strategy=IntervalStrategy.STEPS,\n",
      "save_total_limit=1,\n",
      "seed=0,\n",
      "skip_memory_metrics=True,\n",
      "split_batches=None,\n",
      "tf32=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torchdynamo=None,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "use_cpu=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_mps_device=False,\n",
      "warmup_ratio=0.2,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.0,\n",
      ")\n",
      "2024-06-03 13:27:31 - INFO - __main__ - PEFT parameters LoraConfig(peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path=None, revision=None, task_type='CAUSAL_LM', inference_mode=False, r=16, target_modules='all-linear', lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=None, init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', loftq_config={}, use_dora=False, layer_replication=None)\n"
     ]
    }
   ],
   "source": [
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(levelname)s - %(name)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    handlers=[logging.StreamHandler(sys.stdout)],\n",
    ")\n",
    "log_level = train_conf.get_process_log_level()\n",
    "logger.setLevel(log_level)\n",
    "datasets.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.set_verbosity(log_level)\n",
    "transformers.utils.logging.enable_default_handler()\n",
    "transformers.utils.logging.enable_explicit_format()\n",
    "\n",
    "# Log on each process a small summary\n",
    "logger.warning(\n",
    "    f\"Process rank: {train_conf.local_rank}, device: {train_conf.device}, n_gpu: {train_conf.n_gpu}\"\n",
    "    + f\" distributed training: {bool(train_conf.local_rank != -1)}, 16-bits training: {train_conf.fp16}\"\n",
    ")\n",
    "logger.info(f\"Training/evaluation parameters {train_conf}\")\n",
    "logger.info(f\"PEFT parameters {peft_conf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-06-03 13:27:31,861 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 13:27:32,075 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 13:27:32,076 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"microsoft/Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|modeling_utils.py:3474] 2024-06-03 13:27:32,393 >> loading weights file model.safetensors from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\model.safetensors.index.json\n",
      "[INFO|modeling_utils.py:1519] 2024-06-03 13:27:32,396 >> Instantiating Phi3ForCausalLM model under default dtype torch.bfloat16.\n",
      "[WARNING|logging.py:329] 2024-06-03 13:27:32,398 >> You are attempting to use Flash Attention 2.0 with a model not initialized on GPU. Make sure to move the model to GPU after initializing it on CPU with `model.to('cuda')`.\n",
      "[INFO|configuration_utils.py:962] 2024-06-03 13:27:32,400 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"use_cache\": false\n",
      "}\n",
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "891677e21ed541488aab714c2dabb970",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|modeling_utils.py:4280] 2024-06-03 13:27:42,389 >> All model checkpoint weights were used when initializing Phi3ForCausalLM.\n",
      "\n",
      "[INFO|modeling_utils.py:4288] 2024-06-03 13:27:42,390 >> All the weights of Phi3ForCausalLM were initialized from the model checkpoint at microsoft/Phi-3-mini-128k-instruct.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use Phi3ForCausalLM for predictions without further training.\n",
      "[INFO|configuration_utils.py:917] 2024-06-03 13:27:42,640 >> loading configuration file generation_config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\generation_config.json\n",
      "[INFO|configuration_utils.py:962] 2024-06-03 13:27:42,642 >> Generate config GenerationConfig {\n",
      "  \"bos_token_id\": 1,\n",
      "  \"eos_token_id\": [\n",
      "    32000,\n",
      "    32001,\n",
      "    32007\n",
      "  ],\n",
      "  \"pad_token_id\": 32000\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 13:27:42,868 >> loading file tokenizer.model from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\tokenizer.model\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 13:27:42,869 >> loading file tokenizer.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\tokenizer.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 13:27:42,869 >> loading file added_tokens.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\added_tokens.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 13:27:42,870 >> loading file special_tokens_map.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\special_tokens_map.json\n",
      "[INFO|tokenization_utils_base.py:2108] 2024-06-03 13:27:42,870 >> loading file tokenizer_config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\tokenizer_config.json\n",
      "[WARNING|logging.py:314] 2024-06-03 13:27:42,928 >> Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "################\n",
    "# Modle Loading\n",
    "################\n",
    "#checkpoint_path = \"microsoft/Phi-3-mini-4k-instruct\"\n",
    "checkpoint_path = \"microsoft/Phi-3-mini-128k-instruct\"\n",
    "model_kwargs = dict(\n",
    "    use_cache=False,\n",
    "    trust_remote_code=True,\n",
    "    attn_implementation=\"flash_attention_2\",  # loading the model with flash-attenstion support\n",
    "    torch_dtype=torch.bfloat16,\n",
    "    device_map=None\n",
    ")\n",
    "model = AutoModelForCausalLM.from_pretrained(checkpoint_path, **model_kwargs)\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "tokenizer.model_max_length = 512\n",
    "tokenizer.pad_token = tokenizer.unk_token  # use unk rather than eos token to prevent endless generation\n",
    "tokenizer.pad_token_id = tokenizer.convert_tokens_to_ids(tokenizer.pad_token)\n",
    "tokenizer.padding_side = 'right'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.builder - Overwrite dataset info from restored data version if exists.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from C:\\Users\\acer alan\\.cache\\huggingface\\datasets/wenlianghuang___dataset_phi3_matt_testing/default/0.0.0/a1fb0788e4ae946b0027f6f1318161a2c26c4282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.info - Loading Dataset info from C:\\Users\\acer alan\\.cache\\huggingface\\datasets/wenlianghuang___dataset_phi3_matt_testing/default/0.0.0/a1fb0788e4ae946b0027f6f1318161a2c26c4282\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Found cached dataset dataset_phi3_matt_testing (C:/Users/acer alan/.cache/huggingface/datasets/wenlianghuang___dataset_phi3_matt_testing/default/0.0.0/a1fb0788e4ae946b0027f6f1318161a2c26c4282)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.builder - Found cached dataset dataset_phi3_matt_testing (C:/Users/acer alan/.cache/huggingface/datasets/wenlianghuang___dataset_phi3_matt_testing/default/0.0.0/a1fb0788e4ae946b0027f6f1318161a2c26c4282)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset info from C:/Users/acer alan/.cache/huggingface/datasets/wenlianghuang___dataset_phi3_matt_testing/default/0.0.0/a1fb0788e4ae946b0027f6f1318161a2c26c4282\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.info - Loading Dataset info from C:/Users/acer alan/.cache/huggingface/datasets/wenlianghuang___dataset_phi3_matt_testing/default/0.0.0/a1fb0788e4ae946b0027f6f1318161a2c26c4282\n"
     ]
    }
   ],
   "source": [
    "##################\n",
    "# Data Processing\n",
    "##################\n",
    "def apply_chat_template(\n",
    "    example,\n",
    "    tokenizer,\n",
    "):\n",
    "    messages = example[\"messages\"]\n",
    "    # Add an empty system message if there is none\n",
    "    if messages[0][\"role\"] != \"system\":\n",
    "        messages.insert(0, {\"role\": \"system\", \"content\": \"\"})\n",
    "    example[\"text\"] = tokenizer.apply_chat_template(\n",
    "        messages, tokenize=False, add_generation_prompt=False)\n",
    "    return example\n",
    "\n",
    "raw_dataset = load_dataset(\"wenlianghuang/dataset_phi3_matt_testing\")\n",
    "train_dataset = raw_dataset[\"train_sft\"]\n",
    "test_dataset = raw_dataset[\"test_sft\"]\n",
    "column_names = list(train_dataset.features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00000_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.arrow_dataset - Process #0 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00000_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00001_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.arrow_dataset - Process #1 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00001_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00002_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.arrow_dataset - Process #2 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00002_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00003_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.arrow_dataset - Process #3 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00003_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00004_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.arrow_dataset - Process #4 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-61068def4bd2125e_00004_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spawning 5 processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:51 - INFO - datasets.arrow_dataset - Spawning 5 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "db129d11682c4455b2cf2cea3c0c6dd6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to train_sft (num_proc=5):   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 5 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Concatenating 5 shards\n"
     ]
    }
   ],
   "source": [
    "processed_train_dataset = train_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=5,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to train_sft\",\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #0 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00000_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Process #0 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00000_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #1 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00001_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Process #1 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00001_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #2 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00002_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Process #2 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00002_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #3 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00003_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Process #3 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00003_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process #4 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00004_of_00005.arrow\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Process #4 will write at C:\\Users\\acer alan\\.cache\\huggingface\\datasets\\wenlianghuang___dataset_phi3_matt_testing\\default\\0.0.0\\a1fb0788e4ae946b0027f6f1318161a2c26c4282\\cache-825ef8715f20ed9d_00004_of_00005.arrow\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Spawning 5 processes\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:27:57 - INFO - datasets.arrow_dataset - Spawning 5 processes\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a88c31b22b874696a5b09de29c5623d6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Applying chat template to test_sft (num_proc=5):   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Concatenating 5 shards\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:00 - INFO - datasets.arrow_dataset - Concatenating 5 shards\n"
     ]
    }
   ],
   "source": [
    "processed_test_dataset = test_dataset.map(\n",
    "    apply_chat_template,\n",
    "    fn_kwargs={\"tokenizer\": tokenizer},\n",
    "    num_proc=5,\n",
    "    remove_columns=column_names,\n",
    "    desc=\"Applying chat template to test_sft\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-8fe75ed2c3a85b51\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:01 - INFO - datasets.builder - Using custom data configuration default-8fe75ed2c3a85b51\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\packaged_modules\\generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:01 - INFO - datasets.info - Loading Dataset Infos from c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\packaged_modules\\generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset generator (C:/Users/acer alan/.cache/huggingface/datasets/generator/default-8fe75ed2c3a85b51/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:01 - INFO - datasets.builder - Generating dataset generator (C:/Users/acer alan/.cache/huggingface/datasets/generator/default-8fe75ed2c3a85b51/0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-8fe75ed2c3a85b51/0.0.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:01 - INFO - datasets.builder - Downloading and preparing dataset generator/default to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-8fe75ed2c3a85b51/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:01 - INFO - datasets.builder - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1267d1cb368c4cb2b3afedd2f8152dbc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|tokenization_utils_base.py:3921] 2024-06-03 13:28:02,401 >> Token indices sequence length is longer than the specified maximum sequence length for this model (727 > 512). Running this sequence through the model will result in indexing errors\n",
      "Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-8fe75ed2c3a85b51/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.builder - Dataset generator downloaded and prepared to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-8fe75ed2c3a85b51/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-7facf7caec1a9c35\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.builder - Using custom data configuration default-7facf7caec1a9c35\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\packaged_modules\\generator\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.info - Loading Dataset Infos from c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\datasets\\packaged_modules\\generator\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating dataset generator (C:/Users/acer alan/.cache/huggingface/datasets/generator/default-7facf7caec1a9c35/0.0.0)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.builder - Generating dataset generator (C:/Users/acer alan/.cache/huggingface/datasets/generator/default-7facf7caec1a9c35/0.0.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading and preparing dataset generator/default to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-7facf7caec1a9c35/0.0.0...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.builder - Downloading and preparing dataset generator/default to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-7facf7caec1a9c35/0.0.0...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:09 - INFO - datasets.builder - Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7895cb229d344b895382b6911a02ca9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:11 - INFO - datasets.utils.info_utils - Unable to verify splits sizes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Dataset generator downloaded and prepared to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-7facf7caec1a9c35/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2024-06-03 13:28:11 - INFO - datasets.builder - Dataset generator downloaded and prepared to C:/Users/acer alan/.cache/huggingface/datasets/generator/default-7facf7caec1a9c35/0.0.0. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:641] 2024-06-03 13:28:17,911 >> Using auto half precision backend\n",
      "[INFO|trainer.py:2078] 2024-06-03 13:28:18,055 >> ***** Running training *****\n",
      "[INFO|trainer.py:2079] 2024-06-03 13:28:18,056 >>   Num examples = 6,706\n",
      "[INFO|trainer.py:2080] 2024-06-03 13:28:18,057 >>   Num Epochs = 1\n",
      "[INFO|trainer.py:2081] 2024-06-03 13:28:18,057 >>   Instantaneous batch size per device = 4\n",
      "[INFO|trainer.py:2084] 2024-06-03 13:28:18,057 >>   Total train batch size (w. parallel, distributed & accumulation) = 4\n",
      "[INFO|trainer.py:2085] 2024-06-03 13:28:18,058 >>   Gradient Accumulation steps = 1\n",
      "[INFO|trainer.py:2086] 2024-06-03 13:28:18,058 >>   Total optimization steps = 1,677\n",
      "[INFO|trainer.py:2087] 2024-06-03 13:28:18,061 >>   Number of trainable parameters = 25,165,824\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8fd2bc54bd94f3691b3926b33c65e5e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1677 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.3391, 'grad_norm': 0.859375, 'learning_rate': 2.9761904761904765e-07, 'epoch': 0.01}\n",
      "{'loss': 1.3358, 'grad_norm': 0.9765625, 'learning_rate': 5.952380952380953e-07, 'epoch': 0.02}\n",
      "{'loss': 1.2273, 'grad_norm': 0.484375, 'learning_rate': 8.928571428571429e-07, 'epoch': 0.04}\n",
      "{'loss': 1.2731, 'grad_norm': 0.6015625, 'learning_rate': 1.1904761904761906e-06, 'epoch': 0.05}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 13:33:26,084 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2757, 'grad_norm': 0.4921875, 'learning_rate': 1.4880952380952381e-06, 'epoch': 0.06}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 13:33:27,800 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 13:33:27,803 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 13:33:27,909 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-100\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 13:33:27,911 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-100\\special_tokens_map.json\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2827, 'grad_norm': 0.5859375, 'learning_rate': 1.7857142857142859e-06, 'epoch': 0.07}\n",
      "{'loss': 1.2206, 'grad_norm': 0.52734375, 'learning_rate': 2.0833333333333334e-06, 'epoch': 0.08}\n",
      "{'loss': 1.2604, 'grad_norm': 0.66015625, 'learning_rate': 2.380952380952381e-06, 'epoch': 0.1}\n",
      "{'loss': 1.2433, 'grad_norm': 0.412109375, 'learning_rate': 2.6785714285714285e-06, 'epoch': 0.11}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 13:38:39,316 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2834, 'grad_norm': 0.6015625, 'learning_rate': 2.9761904761904763e-06, 'epoch': 0.12}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 13:38:40,830 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 13:38:40,832 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 13:38:40,881 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-200\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 13:38:40,882 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-200\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 13:38:41,003 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.263, 'grad_norm': 0.69140625, 'learning_rate': 3.273809523809524e-06, 'epoch': 0.13}\n",
      "{'loss': 1.2525, 'grad_norm': 0.3984375, 'learning_rate': 3.5714285714285718e-06, 'epoch': 0.14}\n",
      "{'loss': 1.2064, 'grad_norm': 0.298828125, 'learning_rate': 3.869047619047619e-06, 'epoch': 0.16}\n",
      "{'loss': 1.1364, 'grad_norm': 0.431640625, 'learning_rate': 4.166666666666667e-06, 'epoch': 0.17}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 13:44:00,304 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1594, 'grad_norm': 0.55078125, 'learning_rate': 4.464285714285715e-06, 'epoch': 0.18}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 13:44:01,619 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 13:44:01,623 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 13:44:01,715 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-300\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 13:44:01,716 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-300\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 13:44:02,062 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1471, 'grad_norm': 0.291015625, 'learning_rate': 4.761904761904762e-06, 'epoch': 0.19}\n",
      "{'loss': 1.138, 'grad_norm': 0.2578125, 'learning_rate': 4.99989023370455e-06, 'epoch': 0.2}\n",
      "{'loss': 1.1786, 'grad_norm': 0.232421875, 'learning_rate': 4.996049425354717e-06, 'epoch': 0.21}\n",
      "{'loss': 1.2042, 'grad_norm': 0.8671875, 'learning_rate': 4.986729937340083e-06, 'epoch': 0.23}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 13:49:28,129 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1528, 'grad_norm': 0.29296875, 'learning_rate': 4.971952225381176e-06, 'epoch': 0.24}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 13:49:29,251 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 13:49:29,254 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 13:49:29,395 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-400\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 13:49:29,398 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-400\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 13:49:29,655 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1932, 'grad_norm': 0.26953125, 'learning_rate': 4.951748725674643e-06, 'epoch': 0.25}\n",
      "{'loss': 1.1587, 'grad_norm': 0.328125, 'learning_rate': 4.9261637836977315e-06, 'epoch': 0.26}\n",
      "{'loss': 1.2024, 'grad_norm': 0.2119140625, 'learning_rate': 4.895253556872611e-06, 'epoch': 0.27}\n",
      "{'loss': 1.1471, 'grad_norm': 0.2236328125, 'learning_rate': 4.8590858913041775e-06, 'epoch': 0.29}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 13:54:53,063 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.137, 'grad_norm': 0.291015625, 'learning_rate': 4.817740172861903e-06, 'epoch': 0.3}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 13:54:54,287 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 13:54:54,288 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 13:54:54,381 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-500\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 13:54:54,383 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-500\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 13:54:54,568 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1693, 'grad_norm': 0.234375, 'learning_rate': 4.771307152932579e-06, 'epoch': 0.31}\n",
      "{'loss': 1.1901, 'grad_norm': 0.28125, 'learning_rate': 4.719888749226442e-06, 'epoch': 0.32}\n",
      "{'loss': 1.1139, 'grad_norm': 0.28125, 'learning_rate': 4.663597822073865e-06, 'epoch': 0.33}\n",
      "{'loss': 1.1683, 'grad_norm': 0.26953125, 'learning_rate': 4.602557926703675e-06, 'epoch': 0.35}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:00:03,419 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1746, 'grad_norm': 0.375, 'learning_rate': 4.536903042046778e-06, 'epoch': 0.36}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:00:04,982 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:00:04,985 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:00:05,138 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-600\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:00:05,139 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-600\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:00:05,678 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1092, 'grad_norm': 0.216796875, 'learning_rate': 4.4667772766604065e-06, 'epoch': 0.37}\n",
      "{'loss': 1.125, 'grad_norm': 0.392578125, 'learning_rate': 4.392334552418421e-06, 'epoch': 0.38}\n",
      "{'loss': 1.1584, 'grad_norm': 0.25390625, 'learning_rate': 4.313738266661979e-06, 'epoch': 0.39}\n",
      "{'loss': 1.1235, 'grad_norm': 0.2216796875, 'learning_rate': 4.231160933552109e-06, 'epoch': 0.41}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:05:11,954 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-700\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.2566, 'grad_norm': 0.330078125, 'learning_rate': 4.144783805411415e-06, 'epoch': 0.42}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:05:13,103 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:05:13,106 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:05:13,179 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-700\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:05:13,181 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-700\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:05:13,767 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-600] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.164, 'grad_norm': 0.208984375, 'learning_rate': 4.054796474886038e-06, 'epoch': 0.43}\n",
      "{'loss': 1.1195, 'grad_norm': 0.248046875, 'learning_rate': 3.961396458801099e-06, 'epoch': 0.44}\n",
      "{'loss': 1.1012, 'grad_norm': 0.255859375, 'learning_rate': 3.864788764623042e-06, 'epoch': 0.45}\n",
      "{'loss': 1.1042, 'grad_norm': 0.4296875, 'learning_rate': 3.7651854404804757e-06, 'epoch': 0.47}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:10:27,247 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-800\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1627, 'grad_norm': 0.1953125, 'learning_rate': 3.662805109731168e-06, 'epoch': 0.48}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:10:28,374 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:10:28,377 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:10:28,463 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-800\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:10:28,465 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-800\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:10:28,719 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-700] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1711, 'grad_norm': 0.2119140625, 'learning_rate': 3.557872491096812e-06, 'epoch': 0.49}\n",
      "{'loss': 1.1929, 'grad_norm': 0.322265625, 'learning_rate': 3.450617905418834e-06, 'epoch': 0.5}\n",
      "{'loss': 1.0958, 'grad_norm': 0.193359375, 'learning_rate': 3.341276770117877e-06, 'epoch': 0.51}\n",
      "{'loss': 1.1335, 'grad_norm': 0.2294921875, 'learning_rate': 3.2300890824665942e-06, 'epoch': 0.52}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:15:37,411 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-900\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.137, 'grad_norm': 0.3125, 'learning_rate': 3.117298892809953e-06, 'epoch': 0.54}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:15:38,616 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:15:38,620 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:15:38,703 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-900\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:15:38,706 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-900\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:15:38,935 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-800] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1752, 'grad_norm': 0.171875, 'learning_rate': 3.003153768889276e-06, 'epoch': 0.55}\n",
      "{'loss': 1.1044, 'grad_norm': 0.1826171875, 'learning_rate': 2.887904252445806e-06, 'epoch': 0.56}\n",
      "{'loss': 1.1124, 'grad_norm': 0.49609375, 'learning_rate': 2.7718033092965267e-06, 'epoch': 0.57}\n",
      "{'loss': 1.2478, 'grad_norm': 0.25390625, 'learning_rate': 2.655105774089278e-06, 'epoch': 0.58}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:20:47,684 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1365, 'grad_norm': 0.20703125, 'learning_rate': 2.538067790955892e-06, 'epoch': 0.6}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:20:48,967 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:20:48,969 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:20:49,036 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1000\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:20:49,038 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1000\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:20:49,666 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-900] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0598, 'grad_norm': 0.2578125, 'learning_rate': 2.420946251291103e-06, 'epoch': 0.61}\n",
      "{'loss': 1.1299, 'grad_norm': 0.2109375, 'learning_rate': 2.303998229891249e-06, 'epoch': 0.62}\n",
      "{'loss': 1.148, 'grad_norm': 0.29296875, 'learning_rate': 2.18748042069042e-06, 'epoch': 0.63}\n",
      "{'loss': 1.1469, 'grad_norm': 0.2412109375, 'learning_rate': 2.0716485733325834e-06, 'epoch': 0.64}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:25:57,247 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1100\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1275, 'grad_norm': 0.27734375, 'learning_rate': 1.95675693181636e-06, 'epoch': 0.66}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:25:58,660 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:25:58,661 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:25:58,765 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1100\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:25:58,766 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1100\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:25:58,969 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-1000] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1711, 'grad_norm': 0.25, 'learning_rate': 1.8430576764446046e-06, 'epoch': 0.67}\n",
      "{'loss': 1.1191, 'grad_norm': 0.2412109375, 'learning_rate': 1.730800370303683e-06, 'epoch': 0.68}\n",
      "{'loss': 1.2033, 'grad_norm': 0.328125, 'learning_rate': 1.6202314114873693e-06, 'epoch': 0.69}\n",
      "{'loss': 1.1747, 'grad_norm': 0.24609375, 'learning_rate': 1.51159349226773e-06, 'epoch': 0.7}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:31:05,589 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1200\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1467, 'grad_norm': 0.24609375, 'learning_rate': 1.4051250664000515e-06, 'epoch': 0.72}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:31:07,020 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:31:07,023 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:31:07,105 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1200\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:31:07,106 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1200\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:31:07,311 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-1100] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1213, 'grad_norm': 0.21484375, 'learning_rate': 1.3010598257310642e-06, 'epoch': 0.73}\n",
      "{'loss': 1.1539, 'grad_norm': 0.423828125, 'learning_rate': 1.1996261872592754e-06, 'epoch': 0.74}\n",
      "{'loss': 1.0518, 'grad_norm': 0.296875, 'learning_rate': 1.1010467917732783e-06, 'epoch': 0.75}\n",
      "{'loss': 1.1907, 'grad_norm': 0.263671875, 'learning_rate': 1.005538015168487e-06, 'epoch': 0.76}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:36:17,182 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1300\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0732, 'grad_norm': 0.2109375, 'learning_rate': 9.133094935149592e-07, 'epoch': 0.78}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:36:18,361 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:36:18,363 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:36:18,456 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1300\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:36:18,458 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1300\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:36:18,698 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-1200] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1658, 'grad_norm': 0.177734375, 'learning_rate': 8.245636629187121e-07, 'epoch': 0.79}\n",
      "{'loss': 1.0766, 'grad_norm': 0.19921875, 'learning_rate': 7.394953151865444e-07, 'epoch': 0.8}\n",
      "{'loss': 1.1737, 'grad_norm': 0.208984375, 'learning_rate': 6.582911702696334e-07, 'epoch': 0.81}\n",
      "{'loss': 1.0915, 'grad_norm': 0.212890625, 'learning_rate': 5.811294664243752e-07, 'epoch': 0.82}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:41:26,529 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1312, 'grad_norm': 0.1884765625, 'learning_rate': 5.081795689900398e-07, 'epoch': 0.83}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:41:27,395 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:41:27,396 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:41:27,444 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1400\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:41:27,445 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1400\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:41:27,570 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-1300] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1867, 'grad_norm': 0.181640625, 'learning_rate': 4.396015986419483e-07, 'epoch': 0.85}\n",
      "{'loss': 1.1985, 'grad_norm': 0.1904296875, 'learning_rate': 3.7554607993613823e-07, 'epoch': 0.86}\n",
      "{'loss': 1.1426, 'grad_norm': 0.25, 'learning_rate': 3.1615361091693694e-07, 'epoch': 0.87}\n",
      "{'loss': 1.1924, 'grad_norm': 0.2060546875, 'learning_rate': 2.615545545126416e-07, 'epoch': 0.88}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:46:54,204 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1500\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1344, 'grad_norm': 0.21484375, 'learning_rate': 2.118687523966559e-07, 'epoch': 0.89}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:46:55,940 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:46:55,942 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:46:56,083 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1500\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:46:56,085 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1500\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:46:56,365 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-1400] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.153, 'grad_norm': 0.2138671875, 'learning_rate': 1.6720526194217186e-07, 'epoch': 0.91}\n",
      "{'loss': 1.1558, 'grad_norm': 0.1708984375, 'learning_rate': 1.2766211684773156e-07, 'epoch': 0.92}\n",
      "{'loss': 1.1415, 'grad_norm': 0.21484375, 'learning_rate': 9.332611195910585e-08, 'epoch': 0.93}\n",
      "{'loss': 1.1919, 'grad_norm': 0.349609375, 'learning_rate': 6.427261275978369e-08, 'epoch': 0.94}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 14:52:05,187 >> Saving model checkpoint to ./sample_phi3_finetune_example\\checkpoint-1600\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.0785, 'grad_norm': 0.2353515625, 'learning_rate': 4.056538994822945e-08, 'epoch': 0.95}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 14:52:06,506 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 14:52:06,509 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 14:52:06,593 >> tokenizer config file saved in ./sample_phi3_finetune_example\\checkpoint-1600\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 14:52:06,595 >> Special tokens file saved in ./sample_phi3_finetune_example\\checkpoint-1600\\special_tokens_map.json\n",
      "[INFO|trainer.py:3502] 2024-06-03 14:52:06,845 >> Deleting older checkpoint [sample_phi3_finetune_example\\checkpoint-1500] due to args.save_total_limit\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'loss': 1.1849, 'grad_norm': 0.228515625, 'learning_rate': 2.2256479464999315e-08, 'epoch': 0.97}\n",
      "{'loss': 1.1015, 'grad_norm': 0.32421875, 'learning_rate': 9.386068276959204e-09, 'epoch': 0.98}\n",
      "{'loss': 1.1445, 'grad_norm': 0.17578125, 'learning_rate': 1.982406169283857e-09, 'epoch': 0.99}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:2329] 2024-06-03 14:56:01,058 >> \n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'train_runtime': 5262.9957, 'train_samples_per_second': 1.274, 'train_steps_per_second': 0.319, 'train_loss': 1.1665670079849415, 'epoch': 1.0}\n",
      "***** train metrics *****\n",
      "  epoch                    =         1.0\n",
      "  total_flos               = 143808611GF\n",
      "  train_loss               =      1.1666\n",
      "  train_runtime            =  1:27:42.99\n",
      "  train_samples_per_second =       1.274\n",
      "  train_steps_per_second   =       0.319\n"
     ]
    }
   ],
   "source": [
    "###########\n",
    "# Training\n",
    "###########\n",
    "trainer = SFTTrainer(\n",
    "    model=model,\n",
    "    args=train_conf,\n",
    "    peft_config=peft_conf,\n",
    "    train_dataset=processed_train_dataset,\n",
    "    eval_dataset=processed_test_dataset,\n",
    "    max_seq_length=1024,\n",
    "    dataset_text_field=\"text\",\n",
    "    tokenizer=tokenizer,\n",
    "    packing=True\n",
    ")\n",
    "train_result = trainer.train()\n",
    "metrics = train_result.metrics\n",
    "trainer.log_metrics(\"train\", metrics)\n",
    "trainer.save_metrics(\"train\", metrics)\n",
    "trainer.save_state()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3719] 2024-06-03 14:56:01,084 >> ***** Running Evaluation *****\n",
      "[INFO|trainer.py:3721] 2024-06-03 14:56:01,085 >>   Num examples = 1328\n",
      "[INFO|trainer.py:3724] 2024-06-03 14:56:01,085 >>   Batch size = 4\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1bc70157be954519bca89c66b7dd6d95",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/332 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =        1.0\n",
      "  eval_loss               =     1.1452\n",
      "  eval_runtime            = 0:05:02.28\n",
      "  eval_samples            =       1000\n",
      "  eval_samples_per_second =      4.393\n",
      "  eval_steps_per_second   =      1.098\n"
     ]
    }
   ],
   "source": [
    "#############\n",
    "# Evaluation\n",
    "#############\n",
    "tokenizer.padding_side = 'left'\n",
    "metrics = trainer.evaluate()\n",
    "metrics[\"eval_samples\"] = len(processed_test_dataset)\n",
    "trainer.log_metrics(\"eval\", metrics)\n",
    "trainer.save_metrics(\"eval\", metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|trainer.py:3410] 2024-06-03 15:01:03,428 >> Saving model checkpoint to ./sample_phi3_finetune_example\n",
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:733] 2024-06-03 15:01:04,276 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--microsoft--Phi-3-mini-128k-instruct\\snapshots\\5be6479b4bc06a081e8f4c6ece294241ccd32dec\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 15:01:04,276 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"Phi-3-mini-128k-instruct\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n",
      "[INFO|tokenization_utils_base.py:2513] 2024-06-03 15:01:04,328 >> tokenizer config file saved in ./sample_phi3_finetune_example\\tokenizer_config.json\n",
      "[INFO|tokenization_utils_base.py:2522] 2024-06-03 15:01:04,330 >> Special tokens file saved in ./sample_phi3_finetune_example\\special_tokens_map.json\n"
     ]
    }
   ],
   "source": [
    "# ############\n",
    "# # Save model\n",
    "# ############\n",
    "trainer.save_model(train_conf.output_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "806c584d1eda4125820f200d3465d91a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(HTML(value='<center> <img\\nsrc=https://huggingface.co/front/assets/huggingface_logo-noborder.sv…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from huggingface_hub import notebook_login\n",
    "notebook_login()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\utils\\hub.py:836: FutureWarning: The `use_auth_token` argument is deprecated and will be removed in v5 of Transformers. Please use `token` instead.\n",
      "  warnings.warn(\n",
      "[INFO|configuration_utils.py:472] 2024-06-03 15:24:32,828 >> Configuration saved in sample_phi3_finetune_example\\config.json\n",
      "[INFO|configuration_utils.py:731] 2024-06-03 15:24:32,829 >> Configuration saved in sample_phi3_finetune_example\\generation_config.json\n",
      "[INFO|modeling_utils.py:2626] 2024-06-03 15:24:40,206 >> The model is bigger than the maximum size per checkpoint (5GB) and is going to be split in 2 checkpoint shards. You can find where each parameters has been saved in the index located at sample_phi3_finetune_example\\model.safetensors.index.json.\n",
      "[INFO|hub.py:759] 2024-06-03 15:24:45,366 >> Uploading the following files to wenlianghuang/sample_phi3_finetune_example: config.json,generation_config.json,model-00001-of-00002.safetensors,model-00002-of-00002.safetensors,model.safetensors.index.json,README.md\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6823bb3f0af54fc2b9d05c7930a7086e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7273061434d04987bd7471da2205630f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Upload 2 LFS files:   0%|          | 0/2 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6be5cbd9257d480b8b784b9510560677",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model-00002-of-00002.safetensors:   0%|          | 0.00/2.71G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/wenlianghuang/sample_phi3_finetune_example/commit/880aeee5a7b4893531bd3994d8aadab14056fa46', commit_message='Training Phi-3', commit_description='', oid='880aeee5a7b4893531bd3994d8aadab14056fa46', pr_url=None, pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.push_to_hub(\"sample_phi3_finetune_example\",\n",
    "                  use_auth_token=True,\n",
    "                  commit_message=\"Training Phi-3\",\n",
    "                  private=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available()else 'cpu')\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|tokenization_auto.py:666] 2024-06-03 16:38:01,896 >> Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "c:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3fcfa6d15bf049f999b768aad341b8c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "config.json:   0%|          | 0.00/3.60k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO|configuration_utils.py:733] 2024-06-03 16:38:02,317 >> loading configuration file config.json from cache at C:\\Users\\acer alan\\.cache\\huggingface\\hub\\models--wenlianghuang--sample_phi3_finetune_example\\snapshots\\e5c922a3b7d6c050381aca3ec1c728746e13689b\\config.json\n",
      "[INFO|configuration_utils.py:796] 2024-06-03 16:38:02,319 >> Model config Phi3Config {\n",
      "  \"_name_or_path\": \"wenlianghuang/sample_phi3_finetune_example\",\n",
      "  \"architectures\": [\n",
      "    \"Phi3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"auto_map\": {\n",
      "    \"AutoConfig\": \"microsoft/Phi-3-mini-128k-instruct--configuration_phi3.Phi3Config\",\n",
      "    \"AutoModelForCausalLM\": \"microsoft/Phi-3-mini-128k-instruct--modeling_phi3.Phi3ForCausalLM\"\n",
      "  },\n",
      "  \"bos_token_id\": 1,\n",
      "  \"embd_pdrop\": 0.0,\n",
      "  \"eos_token_id\": 32000,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 3072,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 8192,\n",
      "  \"max_position_embeddings\": 131072,\n",
      "  \"model_type\": \"phi3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 32,\n",
      "  \"num_key_value_heads\": 32,\n",
      "  \"original_max_position_embeddings\": 4096,\n",
      "  \"pad_token_id\": 32000,\n",
      "  \"resid_pdrop\": 0.0,\n",
      "  \"rms_norm_eps\": 1e-05,\n",
      "  \"rope_scaling\": {\n",
      "    \"long_factor\": [\n",
      "      1.0299999713897705,\n",
      "      1.0499999523162842,\n",
      "      1.0499999523162842,\n",
      "      1.0799999237060547,\n",
      "      1.2299998998641968,\n",
      "      1.2299998998641968,\n",
      "      1.2999999523162842,\n",
      "      1.4499999284744263,\n",
      "      1.5999999046325684,\n",
      "      1.6499998569488525,\n",
      "      1.8999998569488525,\n",
      "      2.859999895095825,\n",
      "      3.68999981880188,\n",
      "      5.419999599456787,\n",
      "      5.489999771118164,\n",
      "      5.489999771118164,\n",
      "      9.09000015258789,\n",
      "      11.579999923706055,\n",
      "      15.65999984741211,\n",
      "      15.769999504089355,\n",
      "      15.789999961853027,\n",
      "      18.360000610351562,\n",
      "      21.989999771118164,\n",
      "      23.079999923706055,\n",
      "      30.009998321533203,\n",
      "      32.35000228881836,\n",
      "      32.590003967285156,\n",
      "      35.56000518798828,\n",
      "      39.95000457763672,\n",
      "      53.840003967285156,\n",
      "      56.20000457763672,\n",
      "      57.95000457763672,\n",
      "      59.29000473022461,\n",
      "      59.77000427246094,\n",
      "      59.920005798339844,\n",
      "      61.190006256103516,\n",
      "      61.96000671386719,\n",
      "      62.50000762939453,\n",
      "      63.3700065612793,\n",
      "      63.48000717163086,\n",
      "      63.48000717163086,\n",
      "      63.66000747680664,\n",
      "      63.850006103515625,\n",
      "      64.08000946044922,\n",
      "      64.760009765625,\n",
      "      64.80001068115234,\n",
      "      64.81001281738281,\n",
      "      64.81001281738281\n",
      "    ],\n",
      "    \"short_factor\": [\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.05,\n",
      "      1.1,\n",
      "      1.1,\n",
      "      1.1500000000000001,\n",
      "      1.2000000000000002,\n",
      "      1.2500000000000002,\n",
      "      1.3000000000000003,\n",
      "      1.3500000000000003,\n",
      "      1.5000000000000004,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.000000000000001,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.0500000000000007,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1000000000000005,\n",
      "      2.1500000000000004,\n",
      "      2.1500000000000004,\n",
      "      2.3499999999999996,\n",
      "      2.549999999999999,\n",
      "      2.5999999999999988,\n",
      "      2.5999999999999988,\n",
      "      2.7499999999999982,\n",
      "      2.849999999999998,\n",
      "      2.849999999999998,\n",
      "      2.9499999999999975\n",
      "    ],\n",
      "    \"type\": \"su\"\n",
      "  },\n",
      "  \"rope_theta\": 10000.0,\n",
      "  \"sliding_window\": 262144,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.41.2\",\n",
      "  \"use_cache\": false,\n",
      "  \"vocab_size\": 32064\n",
      "}\n",
      "\n"
     ]
    },
    {
     "ename": "OSError",
     "evalue": "Can't load tokenizer for 'wenlianghuang/sample_phi3_finetune_example'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'wenlianghuang/sample_phi3_finetune_example' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mOSError\u001b[0m                                   Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[23], line 7\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Define the prompt and prepare the input\u001b[39;00m\n\u001b[0;32m      6\u001b[0m prompt \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mWrite a step-by-step recipe on how to make a vegan black bean soup, including all necessary ingredients and cooking instructions. Please also include any variations or substitutions for dietary restrictions.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 7\u001b[0m tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43mAutoTokenizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mwenlianghuang/sample_phi3_finetune_example\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      8\u001b[0m inputs \u001b[38;5;241m=\u001b[39m tokenizer(prompt, return_tensors\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m, padding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, truncation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     10\u001b[0m \u001b[38;5;66;03m# Check if CUDA (GPU) is available and move model and inputs to the appropriate device\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\models\\auto\\tokenization_auto.py:899\u001b[0m, in \u001b[0;36mAutoTokenizer.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, *inputs, **kwargs)\u001b[0m\n\u001b[0;32m    896\u001b[0m tokenizer_class_py, tokenizer_class_fast \u001b[38;5;241m=\u001b[39m TOKENIZER_MAPPING[\u001b[38;5;28mtype\u001b[39m(config)]\n\u001b[0;32m    898\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_fast \u001b[38;5;129;01mand\u001b[39;00m (use_fast \u001b[38;5;129;01mor\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m--> 899\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtokenizer_class_fast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_pretrained\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpretrained_model_name_or_path\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    900\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    901\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m tokenizer_class_py \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[1;32mc:\\Users\\acer alan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\transformers\\tokenization_utils_base.py:2094\u001b[0m, in \u001b[0;36mPreTrainedTokenizerBase.from_pretrained\u001b[1;34m(cls, pretrained_model_name_or_path, cache_dir, force_download, local_files_only, token, revision, trust_remote_code, *init_inputs, **kwargs)\u001b[0m\n\u001b[0;32m   2091\u001b[0m \u001b[38;5;66;03m# If one passes a GGUF file path to `gguf_file` there is no need for this check as the tokenizer will be\u001b[39;00m\n\u001b[0;32m   2092\u001b[0m \u001b[38;5;66;03m# loaded directly from the GGUF file.\u001b[39;00m\n\u001b[0;32m   2093\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mall\u001b[39m(full_file_name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mfor\u001b[39;00m full_file_name \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m gguf_file:\n\u001b[1;32m-> 2094\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mEnvironmentError\u001b[39;00m(\n\u001b[0;32m   2095\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCan\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt load tokenizer for \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m. If you were trying to load it from \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2096\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mhttps://huggingface.co/models\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, make sure you don\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mt have a local directory with the same name. \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2097\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOtherwise, make sure \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpretrained_model_name_or_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is the correct path to a directory \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2098\u001b[0m         \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontaining all relevant files for a \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m tokenizer.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m   2099\u001b[0m     )\n\u001b[0;32m   2101\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m file_id, file_path \u001b[38;5;129;01min\u001b[39;00m vocab_files\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m   2102\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m file_id \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m resolved_vocab_files:\n",
      "\u001b[1;31mOSError\u001b[0m: Can't load tokenizer for 'wenlianghuang/sample_phi3_finetune_example'. If you were trying to load it from 'https://huggingface.co/models', make sure you don't have a local directory with the same name. Otherwise, make sure 'wenlianghuang/sample_phi3_finetune_example' is the correct path to a directory containing all relevant files for a LlamaTokenizerFast tokenizer."
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from peft import PeftModel\n",
    "\n",
    "# Define the prompt and prepare the input\n",
    "prompt = \"Write a step-by-step recipe on how to make a vegan black bean soup, including all necessary ingredients and cooking instructions. Please also include any variations or substitutions for dietary restrictions.\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"wenlianghuang/sample_phi3_finetune_example\")\n",
    "inputs = tokenizer(prompt, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "\n",
    "# Check if CUDA (GPU) is available and move model and inputs to the appropriate device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# Load the model weights from hub and move to the appropriate device\n",
    "model_id = \"wenlianghuang/sample_phi3_finetune_example\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "trained_model = PeftModel.from_pretrained(model, model_id)\n",
    "trained_model.to(device)\n",
    "\n",
    "# Move inputs to the same device as the model\n",
    "inputs = {key: value.to(device) for key, value in inputs.items()}\n",
    "\n",
    "# Run inference\n",
    "outputs = trained_model.generate(**inputs, max_length=1000)\n",
    "\n",
    "# Decode the outputs\n",
    "text = tokenizer.batch_decode(outputs, skip_special_tokens=True)[0]\n",
    "print(text)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

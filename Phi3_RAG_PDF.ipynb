{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pragnakalp.com/leverage-phi-3-exploring-rag-based-qna-with-microsofts-phi-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install langchain chromadb pypdf openai sentence-transformers accelerate\n",
    "!pip install rapidocr-onnxruntime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader,PyMuPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma,FAISS\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "from langchain.vectorstores.base import VectorStoreRetriever\n",
    "from langchain.vectorstores.utils import DistanceStrategy\n",
    "from langchain.chains import RetrievalQA\n",
    "from transformers import TextIteratorStreamer\n",
    "from threading import Thread\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "embeddings = HuggingFaceEmbeddings(model_kwargs=model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True,)\n",
    "\n",
    "streamer = TextIteratorStreamer(\n",
    "        tokenizer=tokenizer, skip_prompt=True, skip_special_tokens=True, timeout=300.0\n",
    ")\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=600,streamer=streamer)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20240515 Can add multiple pdf into a list\n",
    "# Load the PDF file\n",
    "#pdf_link = \"Weekly Report KYD210.pdf\"\n",
    "pdf_link = \"2106.09685v2.pdf\"\n",
    "#loader = PyPDFLoader(pdf_link, extract_images=False)\n",
    "loader = PyPDFLoader(pdf_link, extract_images=True)\n",
    "# load_and_split => Return List[Document]\n",
    "\n",
    "#This function will always show \"Windows platform detected, try to use DirectML as primary provider\" and the running is very slow(with image)\n",
    "#However; sometime it will not happen this problem(without image)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "#pages = loader.load()\n",
    "\n",
    "# Split data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size = 4000,\n",
    "   chunk_overlap  = 0,\n",
    "   length_function = len,\n",
    "   add_start_index = True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorstore = FAISS.from_documents(\n",
    "        chunks, embeddings, distance_strategy=DistanceStrategy.DOT_PRODUCT\n",
    ")\n",
    "retriever = VectorStoreRetriever(vectorstore=vectorstore, search_kwargs={\"k\": 2})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data into database\n",
    "#db=Chroma.from_documents(chunks,embedding=embeddings,persist_directory=\"test_index\")\n",
    "#db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the database\n",
    "#vectordb = Chroma(persist_directory=\"test_index\", embedding_function = embeddings)\n",
    "\n",
    "# Load the retriver\n",
    "#retriever = vectordb.as_retriever(search_kwargs = {\"k\" : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom prompt template suitable for the Phi-3 model\n",
    "qna_prompt_template=\"\"\"<|system|>\n",
    "You have been provided with the context and a question, try to find out the answer to the question only using the context information. If the answer to the question is not found within the context, return \"I dont know\" as the response.<|end|>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "   template=qna_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Define the QNA chain\n",
    "#chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)\n",
    "chain = RetrievalQA.from_chain_type(\n",
    "        llm=llm,\n",
    "        retriever=retriever,\n",
    "        chain_type_kwargs={\"prompt\": PROMPT},\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask(question):\n",
    "    thread = Thread(target=chain.invoke, kwargs={\"input\": {\"query\": question}})\n",
    "    thread.start()\n",
    "    response = \"\"\n",
    "    for token in streamer:\n",
    "        #pattern = r'^[!@#].*?(Response|response)'\n",
    "        #match = re.search(r':\\s*(.*)', token)\n",
    "        # 使用正則表達式進行匹配\n",
    "        #if re.match(pattern, token):\n",
    "        #    continue\n",
    "        #print(token)# 定義正則表達式模式，以匹配開頭的空白、特殊字元以及 \"Response\" 或 \"response\"\n",
    "        \n",
    "        #if \"-\" in token:\n",
    "        #    continue\n",
    "        #pattern = r'^[\\s!@#-]*?(?:Response|response)[:]'\n",
    "\n",
    "        # 使用 sub() 方法替換匹配的部分為空字符串\n",
    "        #cleaned_string = re.sub(pattern, '', token)\n",
    "\n",
    "        # 如果處理後的字符串不為空，則進行處理\n",
    "        #if cleaned_string.strip():\n",
    "        #    print(\"Processed string:\", cleaned_string)\n",
    "        \n",
    "        #response += cleaned_string\n",
    "        response += token\n",
    "        #yield response.strip()\n",
    "    #match = re.search(r':\\s*(.*)', response)\n",
    "    #if match:\n",
    "        # 如果找到匹配，則取出 \":\" 之後的部分\n",
    "    #    response = match.group(1)\n",
    "    return response.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for answer generation\n",
    "#def ask(question):\n",
    "#   context = retriever.get_relevant_documents(question)\n",
    "#   answer = (chain({\"input_documents\": context, \"question\": question}, return_only_outputs=True))['output_text']\n",
    "#   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the user input and call the function to generate output\n",
    "user_question = input(\"User: \")\n",
    "answer = ask(user_question)\n",
    "answer = (answer.split(\"<|assistant|>\")[-1]).strip()\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

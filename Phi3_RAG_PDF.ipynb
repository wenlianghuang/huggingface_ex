{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# https://www.pragnakalp.com/leverage-phi-3-exploring-rag-based-qna-with-microsofts-phi-3/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install langchain chromadb pypdf openai sentence-transformers accelerate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from langchain.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import Chroma\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from langchain import HuggingFacePipeline\n",
    "from langchain.chains.question_answering import load_qa_chain\n",
    "from langchain.prompts import PromptTemplate\n",
    "\n",
    "model_kwargs = {'device': 'cuda'}\n",
    "embeddings = HuggingFaceEmbeddings(model_kwargs=model_kwargs)\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\")\n",
    "model = AutoModelForCausalLM.from_pretrained(\"microsoft/Phi-3-mini-128k-instruct\", device_map='auto', torch_dtype=\"auto\", trust_remote_code=True,)\n",
    "\n",
    "pipe = pipeline(\"text-generation\", model=model, tokenizer=tokenizer, max_new_tokens=300)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the PDF file\n",
    "pdf_link = \"2106.09685v2.pdf\"\n",
    "loader = PyPDFLoader(pdf_link, extract_images=False)\n",
    "pages = loader.load_and_split()\n",
    "\n",
    "\n",
    "# Split data into chunks\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "   chunk_size = 4000,\n",
    "   chunk_overlap  = 20,\n",
    "   length_function = len,\n",
    "   add_start_index = True,\n",
    ")\n",
    "chunks = text_splitter.split_documents(pages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store data into database\n",
    "db=Chroma.from_documents(chunks,embedding=embeddings,persist_directory=\"test_index\")\n",
    "db.persist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the database\n",
    "vectordb = Chroma(persist_directory=\"test_index\", embedding_function = embeddings)\n",
    "\n",
    "# Load the retriver\n",
    "retriever = vectordb.as_retriever(search_kwargs = {\"k\" : 3})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the custom prompt template suitable for the Phi-3 model\n",
    "qna_prompt_template=\"\"\"<|system|>\n",
    "You have been provided with the context and a question, try to find out the answer to the question only using the context information. If the answer to the question is not found within the context, return \"I dont know\" as the response.<|end|>\n",
    "<|user|>\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}<|end|>\n",
    "<|assistant|>\"\"\"\n",
    "PROMPT = PromptTemplate(\n",
    "   template=qna_prompt_template, input_variables=[\"context\", \"question\"]\n",
    ")\n",
    "\n",
    "# Define the QNA chain\n",
    "chain = load_qa_chain(llm, chain_type=\"stuff\", prompt=PROMPT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A utility function for answer generation\n",
    "def ask(question):\n",
    "   context = retriever.get_relevant_documents(question)\n",
    "   print(context)\n",
    "\n",
    "   answer = (chain({\"input_documents\": context, \"question\": question}, return_only_outputs=True))['output_text']\n",
    "   return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take the user input and call the function to generate output\n",
    "user_question = input(\"User: \")\n",
    "answer = ask(user_question)\n",
    "answer = (answer.split(\"<|assistant|>\")[-1]).strip()\n",
    "print(\"Answer:\", answer)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
